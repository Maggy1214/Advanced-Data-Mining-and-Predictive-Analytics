---
title: "ADM_Assignment_3"
author: "Meghana Udiga"
date: "`r Sys.Date()`"
output:
  word_document: default
  pdf_document: default
---
```{r}

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**=======================PART-A===========================**
  
**Question 1: What is the difference between SVM with hard margin and soft margin?**

Support Vector Machine (SVM) is a machine learning algorithm used for both classification and regression analysis. Two variants of SVM are hard margin and soft margin, which differ in the flexibility allowed in the placement of the decision boundary.

Hard margin SVM aims to find a decision boundary that separates two classes with maximum margin, without any misclassifications or errors. This is only possible if the data is linearly separable, meaning there is a clear gap between the classes. The maximum margin is defined as the minimum distance between the decision boundary and the nearest training data points. Hard margin SVM is sensitive to outliers and overfits when the margin is small.

Soft margin SVM allows some degree of misclassification in the training data by introducing a penalty term for misclassifications. This approach is more flexible in finding a decision boundary that handles non-linearly separable data. The degree of flexibility is controlled by the regularization parameter C, which balances maximizing the margin and minimizing misclassification errors. Larger C values result in smaller margins and fewer misclassifications, while smaller C values lead to larger margins and more misclassifications. Soft margin SVM is preferred in real-world applications because data is often noisy or overlapping, and linearly separable data is rare.

The difference between hard margin and soft margin SVM lies in data separability. Hard margin SVM can find the decision boundary with maximum margin without misclassifications only when data is linearly separable. On the other hand, soft margin SVM is more suitable for non-linearly separable data by allowing some degree of misclassification while finding the decision boundary. Soft margin SVM is also useful for linearly separable data with a small margin, where overfitting or sensitivity to outliers may occur.

In summary, hard margin and soft margin SVM are two SVM variants that approach decision boundary differently. Hard margin SVM aims for maximum margin without misclassifications, while soft margin SVM allows some misclassification to handle non-linearly separable data. Soft margin SVM is preferred in real-world applications because of its flexibility in handling noisy or overlapping data.

**Question 2: What is the role of the cost parameter, C, in SVM (with soft margin) classifiers? **

The cost parameter, C, is a vital component in SVM with soft margin classifiers as it regulates the trade-off between maximizing the margin and minimizing classification errors. As a regularization parameter, it assigns weights to constraints and determines how strictly the model should abide by them.

A lower C value results in a softer margin, permitting more misclassifications, indicating that the model is more tolerant of errors and prioritizes discovering the largest margin. Conversely, a higher C value creates a stricter margin, producing fewer misclassifications, and the model tries to minimize errors in the training set, leading to a smaller margin.

When C is set to infinity, no slack variables exist, and the model strictly follows constraints. Thus, all training data points must be correctly classified, making it akin to the hard margin SVM.

In essence, the cost parameter, C, influences how much the model is penalized for misclassifications, creating a balance between margin size and classification error. The optimal C value is problem-specific, depending on the desired trade-off between model complexity and accuracy. A smaller C value produces a simpler model, while a larger C value generates a more intricate model with higher accuracy in the training set.



**Question 3: Will the following perceptron be activated (2.8 is the activation threshold)**

The perceptron is a linear classification algorithm that employs a threshold function and a weighted sum of inputs to make predictions. The perceptron's output is determined by the sign of the weighted sum, and the threshold function decides whether the output should be +1 or -1.

In this instance, the perceptron's weighted sum is calculated as (0.1 * 0.8) + (1 * 1.1 * (-0.2)) = 0.08 - 2.22 = -2.14. Since the result is negative, the perceptron's activation function will output -1. Hence, the perceptron will not be activated.

**Question4: What is the role of alpha, the learning rate in the delta rule?**

The delta rule is a gradient descent-based learning algorithm that aims to find the best weights to fit the training examples. The learning rate, denoted by alpha, is a crucial hyperparameter that determines the speed and stability of weight updates. A high learning rate causes the weights to update quickly, while a low learning rate leads to more stable changes. The optimal learning rate ensures the algorithm's fastest convergence, so it's often recommended to start with a high learning rate and gradually reduce it to fine-tune the weights.

The size of the gradient considered during gradient descent is controlled by the learning rate, alpha. A high alpha takes a larger part of the current gradient, while a low alpha takes a smaller part. Thus, alpha is named the learning rate because it influences the weight updates' speed based on the input, output, and target. A higher alpha leads to faster weight changes, while a lower alpha results in smoother changes. Typically, using a high learning rate at the beginning helps approach the target and then shifting to a smaller rate results in smoothly reaching the optimal weight values.


**========================PART-B==================**

## Importing the required Packages

```{r}
library(dplyr)
library(ISLR)
library(glmnet)
library(caret)
library(kernlab)
```
## Selecting the required attributes.
```{r}
Carseats_required = Carseats %>% select("Sales", "Price","Advertising","Population","Age","Income","Education")
```

**Question B1. Build a linear SVM regression model to predict Sales based on all other attributes ("Price", "Advertising", "Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to  “svmLinear”. What is the R-squared of the model?**

```{r}
tc = trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(143)
linearsvm = train(Sales~., data = Carseats_required, method = "svmLinear",
 trControl=tc,preProcess = c("center", "scale"),tuneLength = 10)
linearsvm
```
## In the above model, I added a train control section that adds cross validation to the model.
## R-squared value is 0.3532603

**Question B2 - Customize the search grid by checking the model’s performance for C parameter of0.1,.5,1 and 10 using 2 repeats of 5-fold cross validation?** 

```{r}
grid = expand.grid(C = c(0.1,0.5,1,10))

tc2 = trainControl(method = "repeatedcv", number = 5, repeats = 2)

linearsvm_grid = train(Sales~., data = Carseats_required, method = "svmLinear",trControl=tc2,preProcess = c("center", "scale"),tuneGrid = grid,tuneLength = 10)
linearsvm_grid
```
## I have customized the search grid with the given values. The number of folds used is 5 with total repeats of 2. Hence, the best value for c when rmse considered is c=0.1  (rmse= 2.277129).

**Question B3 - Train a neural network model to predict Sales based on all other attributes ("Price", "Advertising","Population", "Age", "Income" and "Education"). Hint: use caret train() with method set to “nnet”. What is the R-square of the model with the best hyper parameters (using default caret search grid) – hint: don’t forget to scale the data?**

```{r}
set.seed(6354)
number_of_folds <- trainControl(method = 'LOOCV', verboseIter = FALSE)
carseats_nnet <- train(Sales~., data = Carseats_required, method = "nnet",
preProcess = c("center", "scale"),trControl = number_of_folds)
```

```{r}
carseats_nnet
```

## The final values(best hyper parameter)used for the model are size=1 and decay=1e-04.The R-squared for the selected value is "NA".

**Question B4 - Consider the following input: Sales=9, Price=6.54, Population=124, Advertising=0, Age=76, Income=110, Education=10 What will be the estimated Sales for this record using the above neuralnet model?**

```{r}
 Sales = c(9)
 Price = c(6.54)
 Population = c(124)
 Advertising = c(0)
 Age = c(76)
 Income = c(110)
 Education= c(10)
 
 testset1 = data.frame(Sales, Price, Population, Advertising, Age, Income, Education)

 Predict_sales = predict(carseats_nnet, testset1)
Predict_sales
```
## The instructions suggest that the Neural Net's prediction for the provided record is only one sale. This prediction is worrisome because the decision tree in the previous task predicted that 9.5 sales would occur with the same record. In my view, using the Keras package and constructing a neural network model with "keras_model_sequential" may yield a more flexible model that could potentially lead to a more favorable result.


